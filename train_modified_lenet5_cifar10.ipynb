{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Simplifed LeNet5 Model to classify CIFAR-10 data\n",
    "\n",
    "\n",
    "\n",
    "LeNet5 is a well know model which was intruced inpaper \"Gradient-based learning applied to document recognitio\" by LeCun in1998. In the paer itis ued to classify theMNIST handscripts numbers. Herewe will train asimplified LeNet5 model to clasify CIFAR-10 data.\n",
    "\n",
    "\n",
    "\n",
    "# CIFAR-10 dataset\n",
    "\n",
    "The CIFAR-10 datase consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images nd 10000 test images. \n",
    "\n",
    "The dataset is divided into fivetrainig batches andone test batch, each with 10000 images. The test batch ontains exactly 1000 randomly-selected images from each class. The training batches contain theremaining mages in random order, but some training batches may contain more images from one clas thananother. Betwen them, the training batches contain exactly 5000 images from each class. \n",
    "\n",
    "\n",
    "Here are the classes in the dataset, as well as 10 random images from each:\n",
    "\n",
    "![CIFAR-10-intro-1.JPG](attachment:CIFAR-10-intro-1.JPG)\n",
    "\n",
    "For more information, please refer to below link.\n",
    "\n",
    "http://www.cs.toronto.edu/~kriz/cifar.html\n",
    "\n",
    "## Section 1 Define Super Training Parameters\n",
    "\n",
    "- epoch: define the iteration of the train\n",
    "- batch_size: define the train batch size. It depends on how large the memory is. CIFAR-10 is a very small images. 200 - 500 should be good. \n",
    "- test_size: define the test batch size.  \n",
    "- learn rate (lr): The start learn rate for Agagrad\n",
    "- keep_prob: the probability of the training parameter\n",
    "- augument: To have a better training effect, the image augument is always True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters were defined.\n",
      "learn Rate = 0.01 drop rate= 0.3  image augument= True\n"
     ]
    }
   ],
   "source": [
    "from cifar10 import cifar10\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tf_general as tfg\n",
    "import numpy as np\n",
    "from train_log import train_log\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "\n",
    "try:\n",
    "    #Super parameter definition\n",
    "    tf.flags.DEFINE_string('f', '', 'kernel')\n",
    "    tf.flags.DEFINE_integer('epoch', 50000, 'epoch')\n",
    "    tf.flags.DEFINE_integer('batch_size',500, 'batch size')\n",
    "    tf.flags.DEFINE_integer('test_size', 500, 'test size')\n",
    "    tf.flags.DEFINE_float('lr', 0.01, 'learning rate')\n",
    "    tf.flags.DEFINE_float('drop_rate', 0.3, 'drop out rate for drop lay')\n",
    "    tf.flags.DEFINE_boolean('augument', True, 'if image augument is applied') \n",
    "    #Other training parameter                        \n",
    "    tf.flags.DEFINE_float('ckpt_frequency', 125, 'frequency to save checkpoint')\n",
    "    tf.flags.DEFINE_boolean('restore', False, 'restore from checkpoint and run test')\n",
    "    print('parameters were defined.')\n",
    "except:\n",
    "    print('parameters have been defined.')\n",
    "\n",
    "print(\"learn Rate =\",FLAGS.lr, \"drop rate=\", FLAGS.drop_rate, \" image augument=\",FLAGS.augument)   \n",
    "CONTINUE = 0\n",
    "RUN = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2. Generate Checkpoint dir and Log dir\n",
    "\n",
    "- Checkpoint dir is saved in variable **../Le-Net5-Log/Le-Net5_CLASS/ckpt_RUN**, if the dir doesn't exist then create it. \n",
    "- Log file dir is saved in variable **../Le-Net5-Log/Le-Net5_CLASS/log_RUN**, if the dir doesn't exist then create it. \n",
    "- data_path is the position of the CIFAR-10 image data\n",
    "The reason to save the model and log outside the project is to avoid effect the git code management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_dir = '../Le-Net5-Log/Le-Net5_MOD/ckpt_'+str(RUN)+'/'\n",
    "if not os.path.exists(ckpt_dir):\n",
    "    os.makedirs(ckpt_dir)\n",
    "\n",
    "log_dir = '../Le-Net5-Log/Le-Net5_MOD/log_'+str(RUN)+'/'\n",
    "log = train_log(log_dir)\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "data_path = '../cifar-10-batches-py/'\n",
    "if not os.path.exists(data_path):\n",
    "    print('The data path doesn\\'t exist. Please check if it is a correct data path.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3. Modified LeNet5 \n",
    "This is a modified LeNet5 model. A lot of details are ignored in the model.Below is the architecture of the LeNet5 we implemented. It is a little bit different from the original one as some detail design were removed.  \n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5(object):\n",
    "    def __init__(self, x, n_class=10, drop_rate=0):\n",
    "        self.input = x\n",
    "        self.n_class = n_class\n",
    "        self.drop_rate = drop_rate\n",
    "        self._build_net()\n",
    "\n",
    "    def _build_net(self):\n",
    "        with tf.name_scope('norm'):    \n",
    "            self.x_norm = tf.nn.l2_normalize(tf.cast(self.input, tf.float32),axis=1)\n",
    "        \n",
    "        with tf.name_scope('conv_1'):\n",
    "            self.conv1 = tfg.conv2d(self.x_norm, 5, 1, 6, 'conv1', 'VALID','RELU')\n",
    "            print('conv_1: ', self.conv1.get_shape())    \n",
    "        \n",
    "        with tf.name_scope('pool_1'):\n",
    "            self.pool1 = tfg.avg_pool(self.conv1, 2, 1, 'pool1', 'VALID')\n",
    "            print('pool_1: ', self.pool1.get_shape())\n",
    "              \n",
    "        with tf.name_scope('conv_2'):\n",
    "            self.conv2 = tfg.conv2d(self.pool1, 5, 1, 16, 'conv2', 'VALID','RELU')\n",
    "            print('conv_2: ', self.conv2.get_shape())\n",
    "        \n",
    "        with tf.name_scope('pool_2'):\n",
    "            self.pool2 = tfg.avg_pool(self.conv2, 2, 1, 'pool2', 'VALID')\n",
    "            print('pool_2: ', self.pool2.get_shape())   \n",
    "            \n",
    "        with tf.name_scope('conv_3'):\n",
    "            self.conv3 = tfg.conv2d(self.pool2, 5, 1, 120, 'conv3', 'VALID','RELU')\n",
    "            print('conv_3:', self.conv3.get_shape())\n",
    "        \n",
    "        with tf.name_scope('flat_1'):\n",
    "            self.flat1, self.flat_dim = tfg.flatten(self.conv3)\n",
    "            print('flat_1:', self.flat1.get_shape())\n",
    "        \n",
    "        with tf.name_scope('fc_2'):\n",
    "            self.fc2 = tfg.fc_layer(self.flat1, self.flat_dim, 84, 'fc2','RELU')\n",
    "            print('fc_2 ', self.fc2.get_shape())\n",
    "        \n",
    "        with tf.name_scope('fc_3'):\n",
    "            self.fc3 = tfg.fc_layer(self.fc2, 84, 10, 'fc4','RELU')\n",
    "            print('fc_3: ', self.fc3.get_shape())\n",
    "\n",
    "        with tf.name_scope('drop_out'):\n",
    "            self.drop1 = tfg.drop_out(self.fc3, self.drop_rate, 'drop_out')\n",
    "            print('drop_out: ', self.drop1.get_shape())\n",
    "\n",
    "        with tf.name_scope('prediction'):\n",
    "            self.prediction = tf.nn.softmax(self.drop1)\n",
    "            print('prediction: ', self.prediction.get_shape())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4. Build the calculation graph\n",
    "\n",
    "### 4.1 Input layer \n",
    "\n",
    "Use the data feeder to provide the training data. Therefore we define a placeholder with the same structure of the input data. \n",
    "The CIFAR-10 data is 60000 RGB images with each size is 32x32. The data structure should be[batchsize, 32,32,3]. Input channels are 3.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('input'):\n",
    "    x = tf.placeholder(tf.float32, [None,32,32,3], name='x_input')\n",
    "    drop_rate = tf.placeholder(tf.float32, name='drop_rate')\n",
    "    y_ = tf.placeholder(tf.int64, [None], name='labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Deifne the LeNet5 object\n",
    "\n",
    "Use the previously defined LeNet5 class to build LeNet5 network.\n",
    "y is the output of the LeNet5 network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/deechean/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "conv_1:  (?, 28, 28, 6)\n",
      "pool_1:  (?, 27, 27, 6)\n",
      "conv_2:  (?, 23, 23, 16)\n",
      "pool_2:  (?, 22, 22, 16)\n",
      "conv_3: (?, 18, 18, 120)\n",
      "flat_1: (?, 38880)\n",
      "fc_2  (?, 84)\n",
      "fc_3:  (?, 10)\n",
      "drop_out:  (?, 10)\n",
      "prediction:  (?, 10)\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope('prediction'):\n",
    "    le_net5 = LeNet5(x, drop_rate)\n",
    "    y = le_net5.prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Calculate the cross entropy as the loss\n",
    "Use the cross entropy as the loss. \n",
    "**cross entropy** is normally used as the loss of the network. \n",
    "$$cross\\_entropy(output, label) = \\sum_i{output_i*log(label_i)}$$\n",
    "\n",
    "tensorflow function **sparse_softmax_cross_entropy_with_logits** calculate the cross entropy with the logits which is one hot output from the network and the label which is a interger number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('cross_entropy'):\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y,\n",
    "                                                                              labels=y_, \n",
    "                                                                              name=\"cross_entropy_per_example\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Use Adagrad to minimize the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('train_step'):\n",
    "    train_step = tf.train.AdagradOptimizer(FLAGS.lr).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Calculate the reduce mean as the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('train_accuracy'):\n",
    "    prediction =tf.argmax(y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast( tf.equal(prediction,y_), tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Now, let's start the training..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set length: 50000\n",
      "test set length: 10000\n",
      ".............................................................................................................................\n",
      "2019-06-17 21:00:08 iter 124, Test accuracy:10.93%\n",
      ".............................................................................................................................WARNING:tensorflow:From /home/deechean/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/training/saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "\n",
      "2019-06-17 21:20:50 iter 249, Test accuracy:11.77%\n",
      "................................................."
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "data = cifar10(data_path);\n",
    "\n",
    "with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver(max_to_keep=1)\n",
    "    if CONTINUE != 0:\n",
    "        model_file=tf.train.latest_checkpoint(ckpt_dir)\n",
    "        saver.restore(sess,model_file)\n",
    "    for i in range(CONTINUE, FLAGS.epoch):\n",
    "        train_image, train_label,_ = data.get_train_batch(FLAGS.batch_size,FLAGS.augument)\n",
    "        loss, _,accuracy_rate = sess.run([cross_entropy, train_step, accuracy], \n",
    "                                         feed_dict={drop_rate: FLAGS.drop_rate, x:train_image, y_:train_label})\n",
    "        print('.',end='')    \n",
    "        log.add_log('train_accuracy',i, accuracy_rate)\n",
    "        log.add_log('train_loss',i, loss)\n",
    "        if (i+1) % FLAGS.ckpt_frequency == 0:  #保存预测模型\n",
    "            saver.save(sess,ckpt_dir+'cifar10_'+str(i+1)+'.ckpt',global_step=i+1)\n",
    "            acc_accuracy = 0\n",
    "            for j in range(int(10000/FLAGS.test_size)):                    \n",
    "                test_image, test_label,test_index = data.get_test_batch(FLAGS.test_size)\n",
    "                accuracy_rate, output = sess.run([accuracy,prediction],\n",
    "                                                 feed_dict={drop_rate: 0, x:test_image, y_:test_label})\n",
    "                acc_accuracy += accuracy_rate\n",
    "                log.add_log('test_index',i, test_index)\n",
    "                log.add_log('output',i, output)\n",
    "            accuracy_rate = acc_accuracy/10000*FLAGS.test_size\n",
    "            print()\n",
    "            print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + \n",
    "                  ' iter ' + str(i) + ', Test accuracy:' +str(round(accuracy_rate*100,2))+'%')\n",
    "            log.add_log('test_accuracy',i, accuracy_rate)\n",
    "            log.SaveToFile()\n",
    "    tf.reset_default_graph()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
